{"cells":[{"cell_type":"markdown","source":["# Scala Version"],"metadata":{}},{"cell_type":"code","source":["%scala \nval cable_df = spark.read.format(\"csv\")\n  .option(\"header\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .load(\"/FileStore/tables/cable-3.csv\")\n\ndisplay(cable_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>viewing_timestamp</th><th>household_id</th><th>ad_id</th></tr></thead><tbody><tr><td>2020-08-12 20:01:20</td><td>1</td><td>1001</td></tr><tr><td>2020-08-17 22:22:15</td><td>1</td><td>1001</td></tr><tr><td>2020-08-21 05:33:33</td><td>1</td><td>1002</td></tr><tr><td>2020-08-22 22:52:28</td><td>2</td><td>1001</td></tr><tr><td>2020-08-30 17:02:06</td><td>2</td><td>1002</td></tr></tbody></table></div>"]}}],"execution_count":2},{"cell_type":"code","source":["%scala\ncable_df.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- viewing_timestamp: string (nullable = true)\n-- household_id: integer (nullable = true)\n-- ad_id: integer (nullable = true)\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["%scala \nval ott_df = spark.read.format(\"csv\")\n  .option(\"header\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .load(\"/FileStore/tables/ott-3.csv\")\n\ndisplay(ott_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>viewing_timestamp</th><th>household_id</th><th>ad_id</th></tr></thead><tbody><tr><td>2020-08-22 22:23:12</td><td>1</td><td>1002</td></tr><tr><td>2020-08-22 13:34:32</td><td>2</td><td>1001</td></tr><tr><td>2020-08-28 05:42:18</td><td>2</td><td>1002</td></tr><tr><td>2020-08-29 21:12:16</td><td>3</td><td>1001</td></tr></tbody></table></div>"]}}],"execution_count":4},{"cell_type":"code","source":["%scala \nval ads_df = spark.read.format(\"csv\")\n  .option(\"header\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .load(\"/FileStore/tables/ads-3.csv\")\n\ndisplay(ads_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ad_id</th><th>advertiser</th></tr></thead><tbody><tr><td>1001</td><td>Nike</td></tr><tr><td>1002</td><td>Nike</td></tr><tr><td>1003</td><td>Coke</td></tr></tbody></table></div>"]}}],"execution_count":5},{"cell_type":"code","source":["%scala\n\nimport org.apache.spark.sql.functions.{lit, from_unixtime, unix_timestamp}\n\nval _all = cable_df.withColumn(\"service\", lit(\"cable\")).union(ott_df.withColumn(\"service\", lit(\"ott\"))).union(cable_df.withColumn(\"service\", lit(\"both\"))).union(ott_df.withColumn(\"service\", lit(\"both\"))).withColumn(\"viewing_month\", from_unixtime(unix_timestamp($\"viewing_timestamp\", \"yyyy-MM-dd HH:mm:ss\"), \"yyyy-MM\")).withColumn(\"hour\", from_unixtime(unix_timestamp($\"viewing_timestamp\", \"yyyy-MM-dd HH:mm:ss\"), \"HH\")).filter(($\"hour\">=20) && ($\"hour\"<=22))\n\nval result_df = _all.join(ads_df, _all(\"ad_id\") === ads_df(\"ad_id\")).groupBy(\"advertiser\", \"viewing_month\", \"service\").count().as(\"ads\")\n\n// val result_df = _all.join(ads_df, _all(\"ad_id\") === ads_df(\"ad_id\")).repartition(numPartitions = 2, $\"advertiser\", $\"viewing_month\", $\"service\").groupBy(\"advertiser\", \"viewing_month\", \"service\").count().as(\"ads\")\n\nresult_df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-------------+-------+-----+\nadvertiser|viewing_month|service|count|\n+----------+-------------+-------+-----+\n      Nike|      2020-08|   both|    5|\n      Nike|      2020-08|  cable|    3|\n      Nike|      2020-08|    ott|    2|\n+----------+-------------+-------+-----+\n\nimport org.apache.spark.sql.functions.{lit, from_unixtime, unix_timestamp}\n_all: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [viewing_timestamp: string, household_id: int ... 4 more fields]\nresult_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [advertiser: string, viewing_month: string ... 2 more fields]\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["%scala\n// default partition 200\nresult_df.explain()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\n*(7) HashAggregate(keys=[advertiser#165, viewing_month#677, service#637], functions=[finalmerge_count(merge count#744L) AS count(1)#722L])\n+- Exchange hashpartitioning(advertiser#165, viewing_month#677, service#637, 200), true, [id=#1513]\n   +- *(6) HashAggregate(keys=[advertiser#165, viewing_month#677, service#637], functions=[partial_count(1) AS count#744L])\n      +- *(6) Project [service#637, viewing_month#677, advertiser#165]\n         +- *(6) BroadcastHashJoin [ad_id#96], [ad_id#164], Inner, BuildRight\n            :- Union\n            :  :- *(1) Project [ad_id#96, cable AS service#637, from_unixtime(unix_timestamp(viewing_timestamp#94, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), yyyy-MM, Some(Etc/UTC)) AS viewing_month#677]\n            :  :  +- *(1) Filter (((cast(from_unixtime(unix_timestamp(viewing_timestamp#94, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), HH, Some(Etc/UTC)) as int) &gt;= 20) AND (cast(from_unixtime(unix_timestamp(viewing_timestamp#94, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), HH, Some(Etc/UTC)) as int) &lt;= 22)) AND isnotnull(ad_id#96))\n            :  :     +- FileScan csv [viewing_timestamp#94,ad_id#96] Batched: false, DataFilters: [(cast(from_unixtime(unix_timestamp(viewing_timestamp#94, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), HH..., Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/cable-3.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ad_id)], ReadSchema: struct&lt;viewing_timestamp:string,ad_id:int&gt;\n            :  :- *(2) Project [ad_id#131, ott AS service#642, from_unixtime(unix_timestamp(viewing_timestamp#129, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), yyyy-MM, Some(Etc/UTC)) AS viewing_month#783]\n            :  :  +- *(2) Filter (((cast(from_unixtime(unix_timestamp(viewing_timestamp#129, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), HH, Some(Etc/UTC)) as int) &gt;= 20) AND (cast(from_unixtime(unix_timestamp(viewing_timestamp#129, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), HH, Some(Etc/UTC)) as int) &lt;= 22)) AND isnotnull(ad_id#131))\n            :  :     +- FileScan csv [viewing_timestamp#129,ad_id#131] Batched: false, DataFilters: [(cast(from_unixtime(unix_timestamp(viewing_timestamp#129, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), H..., Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/ott-3.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ad_id)], ReadSchema: struct&lt;viewing_timestamp:string,ad_id:int&gt;\n            :  :- *(3) Project [ad_id#96, both AS service#659, from_unixtime(unix_timestamp(viewing_timestamp#94, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), yyyy-MM, Some(Etc/UTC)) AS viewing_month#784]\n            :  :  +- *(3) Filter (((cast(from_unixtime(unix_timestamp(viewing_timestamp#94, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), HH, Some(Etc/UTC)) as int) &gt;= 20) AND (cast(from_unixtime(unix_timestamp(viewing_timestamp#94, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), HH, Some(Etc/UTC)) as int) &lt;= 22)) AND isnotnull(ad_id#96))\n            :  :     +- FileScan csv [viewing_timestamp#94,ad_id#96] Batched: false, DataFilters: [(cast(from_unixtime(unix_timestamp(viewing_timestamp#94, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), HH..., Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/cable-3.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ad_id)], ReadSchema: struct&lt;viewing_timestamp:string,ad_id:int&gt;\n            :  +- *(4) Project [ad_id#131, both AS service#672, from_unixtime(unix_timestamp(viewing_timestamp#129, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), yyyy-MM, Some(Etc/UTC)) AS viewing_month#785]\n            :     +- *(4) Filter (((cast(from_unixtime(unix_timestamp(viewing_timestamp#129, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), HH, Some(Etc/UTC)) as int) &gt;= 20) AND (cast(from_unixtime(unix_timestamp(viewing_timestamp#129, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), HH, Some(Etc/UTC)) as int) &lt;= 22)) AND isnotnull(ad_id#131))\n            :        +- FileScan csv [viewing_timestamp#129,ad_id#131] Batched: false, DataFilters: [(cast(from_unixtime(unix_timestamp(viewing_timestamp#129, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), H..., Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/ott-3.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ad_id)], ReadSchema: struct&lt;viewing_timestamp:string,ad_id:int&gt;\n            +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint))), [id=#1507]\n               +- *(5) Project [ad_id#164, advertiser#165]\n                  +- *(5) Filter isnotnull(ad_id#164)\n                     +- FileScan csv [ad_id#164,advertiser#165] Batched: false, DataFilters: [isnotnull(ad_id#164)], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/ads-3.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ad_id)], ReadSchema: struct&lt;ad_id:int,advertiser:string&gt;\n\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["%scala\n// set repartition to 2\nresult_df.explain()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\n*(7) HashAggregate(keys=[advertiser#165, viewing_month#217, service#177], functions=[count(1)])\n+- Exchange hashpartitioning(advertiser#165, viewing_month#217, service#177, 2), false, [id=#769]\n   +- *(6) Project [service#177, viewing_month#217, advertiser#165]\n      +- *(6) BroadcastHashJoin [ad_id#96], [ad_id#164], Inner, BuildRight\n         :- Union\n         :  :- *(1) Project [ad_id#96, cable AS service#177, from_unixtime(unix_timestamp(viewing_timestamp#94, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), yyyy-MM, Some(Etc/UTC)) AS viewing_month#217]\n         :  :  +- *(1) Filter (((cast(from_unixtime(unix_timestamp(viewing_timestamp#94, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), HH, Some(Etc/UTC)) as int) &gt;= 20) AND (cast(from_unixtime(unix_timestamp(viewing_timestamp#94, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), HH, Some(Etc/UTC)) as int) &lt;= 22)) AND isnotnull(ad_id#96))\n         :  :     +- FileScan csv [viewing_timestamp#94,ad_id#96] Batched: false, DataFilters: [(cast(from_unixtime(unix_timestamp(viewing_timestamp#94, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), HH..., Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/cable-3.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ad_id)], ReadSchema: struct&lt;viewing_timestamp:string,ad_id:int&gt;\n         :  :- *(2) Project [ad_id#131, ott AS service#182, from_unixtime(unix_timestamp(viewing_timestamp#129, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), yyyy-MM, Some(Etc/UTC)) AS viewing_month#326]\n         :  :  +- *(2) Filter (((cast(from_unixtime(unix_timestamp(viewing_timestamp#129, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), HH, Some(Etc/UTC)) as int) &gt;= 20) AND (cast(from_unixtime(unix_timestamp(viewing_timestamp#129, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), HH, Some(Etc/UTC)) as int) &lt;= 22)) AND isnotnull(ad_id#131))\n         :  :     +- FileScan csv [viewing_timestamp#129,ad_id#131] Batched: false, DataFilters: [(cast(from_unixtime(unix_timestamp(viewing_timestamp#129, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), H..., Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/ott-3.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ad_id)], ReadSchema: struct&lt;viewing_timestamp:string,ad_id:int&gt;\n         :  :- *(3) Project [ad_id#96, both AS service#199, from_unixtime(unix_timestamp(viewing_timestamp#94, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), yyyy-MM, Some(Etc/UTC)) AS viewing_month#327]\n         :  :  +- *(3) Filter (((cast(from_unixtime(unix_timestamp(viewing_timestamp#94, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), HH, Some(Etc/UTC)) as int) &gt;= 20) AND (cast(from_unixtime(unix_timestamp(viewing_timestamp#94, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), HH, Some(Etc/UTC)) as int) &lt;= 22)) AND isnotnull(ad_id#96))\n         :  :     +- FileScan csv [viewing_timestamp#94,ad_id#96] Batched: false, DataFilters: [(cast(from_unixtime(unix_timestamp(viewing_timestamp#94, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), HH..., Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/cable-3.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ad_id)], ReadSchema: struct&lt;viewing_timestamp:string,ad_id:int&gt;\n         :  +- *(4) Project [ad_id#131, both AS service#212, from_unixtime(unix_timestamp(viewing_timestamp#129, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), yyyy-MM, Some(Etc/UTC)) AS viewing_month#328]\n         :     +- *(4) Filter (((cast(from_unixtime(unix_timestamp(viewing_timestamp#129, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), HH, Some(Etc/UTC)) as int) &gt;= 20) AND (cast(from_unixtime(unix_timestamp(viewing_timestamp#129, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), HH, Some(Etc/UTC)) as int) &lt;= 22)) AND isnotnull(ad_id#131))\n         :        +- FileScan csv [viewing_timestamp#129,ad_id#131] Batched: false, DataFilters: [(cast(from_unixtime(unix_timestamp(viewing_timestamp#129, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)), H..., Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/ott-3.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ad_id)], ReadSchema: struct&lt;viewing_timestamp:string,ad_id:int&gt;\n         +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint))), [id=#764]\n            +- *(5) Project [ad_id#164, advertiser#165]\n               +- *(5) Filter isnotnull(ad_id#164)\n                  +- FileScan csv [ad_id#164,advertiser#165] Batched: false, DataFilters: [isnotnull(ad_id#164)], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/ads-3.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ad_id)], ReadSchema: struct&lt;ad_id:int,advertiser:string&gt;\n\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["%scala\nval permanent_table_name = \"result_scala\"\nresult_df.write.format(\"parquet\").saveAsTable(permanent_table_name)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:685)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:606)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3465052138114507:2)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3465052138114507:88)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3465052138114507:90)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3465052138114507:92)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3465052138114507:94)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3465052138114507:96)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3465052138114507:98)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3465052138114507:100)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3465052138114507:102)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3465052138114507:104)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3465052138114507:106)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3465052138114507:108)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3465052138114507:110)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3465052138114507:112)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3465052138114507:114)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3465052138114507:116)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3465052138114507:118)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3465052138114507:120)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3465052138114507:122)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3465052138114507:124)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3465052138114507:126)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3465052138114507:128)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw$$iw$$iw.&lt;init&gt;(command-3465052138114507:130)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw$$iw.&lt;init&gt;(command-3465052138114507:132)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$$iw.&lt;init&gt;(command-3465052138114507:134)\n\tat linea69ba5e806b3451888794df63395eebd203.$read.&lt;init&gt;(command-3465052138114507:136)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$.&lt;init&gt;(command-3465052138114507:140)\n\tat linea69ba5e806b3451888794df63395eebd203.$read$.&lt;clinit&gt;(command-3465052138114507)\n\tat linea69ba5e806b3451888794df63395eebd203.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat linea69ba5e806b3451888794df63395eebd203.$eval$.$print(&lt;notebook&gt;:6)\n\tat linea69ba5e806b3451888794df63395eebd203.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1021)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:574)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:41)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:37)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:600)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:570)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:215)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:202)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:714)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:667)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:396)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:230)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:268)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:373)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)</div>"]}}],"execution_count":9},{"cell_type":"code","source":["%scala\nval read_parquet_df = spark.read.parquet(\"/user/hive/warehouse/result_scala\")\nread_parquet_df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-------------+-------+-----+\nadvertiser|viewing_month|service|count|\n+----------+-------------+-------+-----+\n      Nike|      2020-08|  cable|    3|\n      Nike|      2020-08|   both|    5|\n      Nike|      2020-08|    ott|    2|\n+----------+-------------+-------+-----+\n\nread_parquet_df: org.apache.spark.sql.DataFrame = [advertiser: string, viewing_month: string ... 2 more fields]\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["# PySpark Version"],"metadata":{}},{"cell_type":"code","source":["# File location and type\nfile_location = \"/FileStore/tables/cable-3.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"false\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ncable_df = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(cable_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>viewing_timestamp</th><th>household_id</th><th>ad_id</th></tr></thead><tbody><tr><td>2020-08-12 20:01:20</td><td>1</td><td>1001</td></tr><tr><td>2020-08-17 22:22:15</td><td>1</td><td>1001</td></tr><tr><td>2020-08-21 05:33:33</td><td>1</td><td>1002</td></tr><tr><td>2020-08-22 22:52:28</td><td>2</td><td>1001</td></tr><tr><td>2020-08-30 17:02:06</td><td>2</td><td>1002</td></tr></tbody></table></div>"]}}],"execution_count":12},{"cell_type":"code","source":["cable_df.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- viewing_timestamp: string (nullable = true)\n-- household_id: string (nullable = true)\n-- ad_id: string (nullable = true)\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["# File location and type\nfile_location = \"/FileStore/tables/ott-3.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"false\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\nott_df = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(ott_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>viewing_timestamp</th><th>household_id</th><th>ad_id</th></tr></thead><tbody><tr><td>2020-08-22 22:23:12</td><td>1</td><td>1002</td></tr><tr><td>2020-08-22 13:34:32</td><td>2</td><td>1001</td></tr><tr><td>2020-08-28 05:42:18</td><td>2</td><td>1002</td></tr><tr><td>2020-08-29 21:12:16</td><td>3</td><td>1001</td></tr></tbody></table></div>"]}}],"execution_count":14},{"cell_type":"code","source":["# File location and type\nfile_location = \"/FileStore/tables/ads-3.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"false\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\nads_df = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(ads_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ad_id</th><th>advertiser</th></tr></thead><tbody><tr><td>1001</td><td>Nike</td></tr><tr><td>1002</td><td>Nike</td></tr><tr><td>1003</td><td>Coke</td></tr></tbody></table></div>"]}}],"execution_count":15},{"cell_type":"code","source":["from pyspark.sql.functions import year, month, dayofmonth, hour, concat, lit\n\n_all = cable_df.withColumn('service', lit('cable'))\\\n.union(ott_df.withColumn('service', lit('ott')))\\\n.union(cable_df.withColumn('service', lit('both')))\\\n.union(ott_df.withColumn('service', lit('both'))).withColumn('viewing_month', concat(year('viewing_timestamp'), lit(\"-\"), month('viewing_timestamp'))).withColumn('hour', hour('viewing_timestamp'))\n\nresult_df = _all.join(ads_df, _all.ad_id == ads_df.ad_id).filter((_all.hour>=20) & (_all.hour<=22)).repartition(2, [\"advertiser\", \"viewing_month\", \"service\"]).groupby([\"advertiser\", \"viewing_month\", \"service\"]).count().alias('ads')\n#result_df = _all.join(ads_df, _all.ad_id == ads_df.ad_id).filter((_all.hour>=20) & (_all.hour<=22)).groupby([\"advertiser\", \"viewing_month\", \"service\"]).count().alias('ads')\n\nresult_df.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-------------+-------+-----+\nadvertiser|viewing_month|service|count|\n+----------+-------------+-------+-----+\n      Nike|       2020-8|    ott|    2|\n      Nike|       2020-8|   both|    5|\n      Nike|       2020-8|  cable|    3|\n+----------+-------------+-------+-----+\n\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["# default\nresult_df.rdd.getNumPartitions()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[96]: 200</div>"]}}],"execution_count":17},{"cell_type":"code","source":["# repartition\nresult_df.rdd.getNumPartitions()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[98]: 2</div>"]}}],"execution_count":18},{"cell_type":"code","source":["result_df.explain()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\n*(7) HashAggregate(keys=[advertiser#124, viewing_month#9929, service#9889], functions=[count(1)])\n+- Exchange hashpartitioning(advertiser#124, viewing_month#9929, service#9889, 2), false, [id=#19150]\n   +- *(6) Project [service#9889, viewing_month#9929, advertiser#124]\n      +- *(6) BroadcastHashJoin [ad_id#61], [ad_id#123], Inner, BuildRight\n         :- Union\n         :  :- *(1) Project [ad_id#61, cable AS service#9889, concat(cast(year(cast(viewing_timestamp#59 as date)) as string), -, cast(month(cast(viewing_timestamp#59 as date)) as string)) AS viewing_month#9929]\n         :  :  +- *(1) Filter (((hour(cast(viewing_timestamp#59 as timestamp), Some(Etc/UTC)) &gt;= 20) AND (hour(cast(viewing_timestamp#59 as timestamp), Some(Etc/UTC)) &lt;= 22)) AND isnotnull(ad_id#61))\n         :  :     +- FileScan csv [viewing_timestamp#59,ad_id#61] Batched: false, DataFilters: [(hour(cast(viewing_timestamp#59 as timestamp), Some(Etc/UTC)) &gt;= 20), (hour(cast(viewing_timesta..., Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/cable-3.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ad_id)], ReadSchema: struct&lt;viewing_timestamp:string,ad_id:string&gt;\n         :  :- *(2) Project [ad_id#93, ott AS service#9894, concat(cast(year(cast(viewing_timestamp#91 as date)) as string), -, cast(month(cast(viewing_timestamp#91 as date)) as string)) AS viewing_month#10038]\n         :  :  +- *(2) Filter (((hour(cast(viewing_timestamp#91 as timestamp), Some(Etc/UTC)) &gt;= 20) AND (hour(cast(viewing_timestamp#91 as timestamp), Some(Etc/UTC)) &lt;= 22)) AND isnotnull(ad_id#93))\n         :  :     +- FileScan csv [viewing_timestamp#91,ad_id#93] Batched: false, DataFilters: [(hour(cast(viewing_timestamp#91 as timestamp), Some(Etc/UTC)) &gt;= 20), (hour(cast(viewing_timesta..., Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/ott-3.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ad_id)], ReadSchema: struct&lt;viewing_timestamp:string,ad_id:string&gt;\n         :  :- *(3) Project [ad_id#61, both AS service#9911, concat(cast(year(cast(viewing_timestamp#59 as date)) as string), -, cast(month(cast(viewing_timestamp#59 as date)) as string)) AS viewing_month#10039]\n         :  :  +- *(3) Filter (((hour(cast(viewing_timestamp#59 as timestamp), Some(Etc/UTC)) &gt;= 20) AND (hour(cast(viewing_timestamp#59 as timestamp), Some(Etc/UTC)) &lt;= 22)) AND isnotnull(ad_id#61))\n         :  :     +- FileScan csv [viewing_timestamp#59,ad_id#61] Batched: false, DataFilters: [(hour(cast(viewing_timestamp#59 as timestamp), Some(Etc/UTC)) &gt;= 20), (hour(cast(viewing_timesta..., Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/cable-3.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ad_id)], ReadSchema: struct&lt;viewing_timestamp:string,ad_id:string&gt;\n         :  +- *(4) Project [ad_id#93, both AS service#9924, concat(cast(year(cast(viewing_timestamp#91 as date)) as string), -, cast(month(cast(viewing_timestamp#91 as date)) as string)) AS viewing_month#10040]\n         :     +- *(4) Filter (((hour(cast(viewing_timestamp#91 as timestamp), Some(Etc/UTC)) &gt;= 20) AND (hour(cast(viewing_timestamp#91 as timestamp), Some(Etc/UTC)) &lt;= 22)) AND isnotnull(ad_id#93))\n         :        +- FileScan csv [viewing_timestamp#91,ad_id#93] Batched: false, DataFilters: [(hour(cast(viewing_timestamp#91 as timestamp), Some(Etc/UTC)) &gt;= 20), (hour(cast(viewing_timesta..., Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/ott-3.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ad_id)], ReadSchema: struct&lt;viewing_timestamp:string,ad_id:string&gt;\n         +- BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[0, string, true])), [id=#19145]\n            +- *(5) Project [ad_id#123, advertiser#124]\n               +- *(5) Filter isnotnull(ad_id#123)\n                  +- FileScan csv [ad_id#123,advertiser#124] Batched: false, DataFilters: [isnotnull(ad_id#123)], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/ads-3.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ad_id)], ReadSchema: struct&lt;ad_id:string,advertiser:string&gt;\n\n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["permanent_table_name = \"result_df_pyspark\"\nresult_df.write.format(\"parquet\").saveAsTable(permanent_table_name)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20},{"cell_type":"code","source":["read_parquet_df = sqlContext.read.parquet('/user/hive/warehouse/result_df_pyspark')\nread_parquet_df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-------------+-------+-----+\nadvertiser|viewing_month|service|count|\n+----------+-------------+-------+-----+\n      Nike|       2020-8|  cable|    3|\n      Nike|       2020-8|   both|    5|\n      Nike|       2020-8|    ott|    2|\n+----------+-------------+-------+-----+\n\n</div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["# SQL Version"],"metadata":{}},{"cell_type":"code","source":["# Create a view or table\n\ntemp_table_name = \"ott_csv\"\n\nott_df.createOrReplaceTempView(temp_table_name)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"code","source":["%sql\n\n/* Query the created temp table in a SQL cell */\n\nselect * from `ott_csv`"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>viewing_timestamp</th><th>household_id</th><th>ad_id</th></tr></thead><tbody><tr><td>2020-08-22 22:23:12</td><td>1</td><td>1002</td></tr><tr><td>2020-08-22 13:34:32</td><td>2</td><td>1001</td></tr><tr><td>2020-08-28 05:42:18</td><td>2</td><td>1002</td></tr><tr><td>2020-08-29 21:12:16</td><td>3</td><td>1001</td></tr></tbody></table></div>"]}}],"execution_count":24},{"cell_type":"code","source":["# Create a view or table\n\ntemp_table_name = \"cable_csv\"\n\ncable_df.createOrReplaceTempView(temp_table_name)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"code","source":["%sql\n\n/* Query the created temp table in a SQL cell */\n\nselect * from `cable_csv`"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>viewing_timestamp</th><th>household_id</th><th>ad_id</th></tr></thead><tbody><tr><td>2020-08-12 20:01:20</td><td>1</td><td>1001</td></tr><tr><td>2020-08-17 22:22:15</td><td>1</td><td>1001</td></tr><tr><td>2020-08-21 05:33:33</td><td>1</td><td>1002</td></tr><tr><td>2020-08-22 22:52:28</td><td>2</td><td>1001</td></tr><tr><td>2020-08-30 17:02:06</td><td>2</td><td>1002</td></tr></tbody></table></div>"]}}],"execution_count":26},{"cell_type":"code","source":["# Create a view or table\n\ntemp_table_name = \"ads_csv\"\n\nads_df.createOrReplaceTempView(temp_table_name)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":27},{"cell_type":"code","source":["%sql\n\n/* Query the created temp table in a SQL cell */\n\nselect * from `ads_csv`"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ad_id</th><th>advertiser</th></tr></thead><tbody><tr><td>1001</td><td>Nike</td></tr><tr><td>1002</td><td>Nike</td></tr><tr><td>1003</td><td>Coke</td></tr></tbody></table></div>"]}}],"execution_count":28},{"cell_type":"code","source":["%sql\nselect *, substring(viewing_timestamp, 12 , 2) as hour from `cable_csv` where int(substring(viewing_timestamp, 12 , 2)) >= 20 and int(substring(viewing_timestamp, 12 , 2)) <= 22"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>viewing_timestamp</th><th>household_id</th><th>ad_id</th><th>hour</th></tr></thead><tbody><tr><td>2020-08-12 20:01:20</td><td>1</td><td>1001</td><td>20</td></tr><tr><td>2020-08-17 22:22:15</td><td>1</td><td>1001</td><td>22</td></tr><tr><td>2020-08-22 22:52:28</td><td>2</td><td>1001</td><td>22</td></tr></tbody></table></div>"]}}],"execution_count":29},{"cell_type":"code","source":["sqlContext.sql(\"set spark.sql.shuffle.partitions=2\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[23]: DataFrame[key: string, value: string]</div>"]}}],"execution_count":30},{"cell_type":"code","source":["%sql\n\n/* Query the created temp table in a SQL cell */\n\nselect advertiser, substring(viewing_timestamp, 0 , 7) as viewing_month, service, count(*) as ads\nfrom \n(\n  select *, int(substring(viewing_timestamp, 12 , 2)) as hour from\n  (\n    select *, 'cable' as service from `cable_csv` union\n    select *, 'ott' as service from `ott_csv` union\n    select *, 'both' as service from `cable_csv` union\n    select *, 'both' as service from `ott_csv`\n   ) as _all join `ads_csv` as ad on _all.ad_id = ad.ad_id \n  where int(substring(viewing_timestamp, 12 , 2)) >= 20 and int(substring(viewing_timestamp, 12 , 2)) <= 22 \n)\ngroup by advertiser, viewing_month, service\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>advertiser</th><th>viewing_month</th><th>service</th><th>ads</th></tr></thead><tbody><tr><td>Nike</td><td>2020-08</td><td>cable</td><td>3</td></tr><tr><td>Nike</td><td>2020-08</td><td>ott</td><td>2</td></tr><tr><td>Nike</td><td>2020-08</td><td>both</td><td>5</td></tr></tbody></table></div>"]}}],"execution_count":31},{"cell_type":"code","source":["result_df = sqlContext.sql(\\\n                           \"select advertiser, substring(viewing_timestamp, 0 , 7) as viewing_month, service, count(*) as ads\\\n                            from \\\n                            (\\\n                              select *, int(substring(viewing_timestamp, 12 , 2)) as hour from\\\n                              (\\\n                                select *, 'cable' as service from `cable_csv` union\\\n                                select *, 'ott' as service from `ott_csv` union\\\n                                select *, 'both' as service from `cable_csv` union\\\n                                select *, 'both' as service from `ott_csv`\\\n                               ) as _all join `ads_csv` as ad on _all.ad_id = ad.ad_id \\\n                              where int(substring(viewing_timestamp, 12 , 2)) >= 20 and int(substring(viewing_timestamp, 12 , 2)) <= 22 \\\n                            )\\\n                            group by advertiser, viewing_month, service\"\\\n                          )\nresult_df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-------------+-------+---+\nadvertiser|viewing_month|service|ads|\n+----------+-------------+-------+---+\n      Nike|      2020-08|   both|  5|\n      Nike|      2020-08|  cable|  3|\n      Nike|      2020-08|    ott|  2|\n+----------+-------------+-------+---+\n\n</div>"]}}],"execution_count":32},{"cell_type":"code","source":["# With this registered as a temp view, it will only be available to this particular notebook. If you'd like other users to be able to query this table, you can also create a table from the DataFrame.\n# Once saved, this table will persist across cluster restarts as well as allow various users across different notebooks to query this data.\n# To do so, choose your table name and uncomment the bottom line.\n\npermanent_table_name = \"result_sql\"\nresult_df.write.format(\"parquet\").saveAsTable(permanent_table_name)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":33}],"metadata":{"name":"spark etl","notebookId":1214606209898627},"nbformat":4,"nbformat_minor":0}
